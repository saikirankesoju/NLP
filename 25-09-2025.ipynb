{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnWMfOrICLeW89u9mUW7cV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saikirankesoju/NLP/blob/main/25-09-2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "end_to_end_sentiment_with_datasets.py\n",
        "\n",
        "- Downloads a public tweet sentiment dataset (Sentiment140) automatically (or uses a small built-in sample if download fails).\n",
        "- Prepares data (maps 4->1 positive, 0->0 negative, drops neutral 2).\n",
        "- Optionally downloads GloVe 6B 300d (if you want embeddings) or uses random embeddings fallback.\n",
        "- Builds embedding matrix (no gensim required).\n",
        "- Trains and evaluates: LSTM, CNN, Bi-LSTM (Keras/TensorFlow).\n",
        "- Trains and evaluates: TF-IDF + LogisticRegression and LinearSVC.\n",
        "- Saves comparison CSV and error analysis CSV (5 misclassified positive and negative examples).\n",
        "- Designed to run on a typical laptop; by default it samples a manageable subset from Sentiment140 (you can increase SAMPLE_SIZE if you have GPU/time).\n",
        "\n",
        "USAGE:\n",
        "- Ensure Python packages installed: pandas, numpy, scikit-learn, requests, tqdm, tensorflow, nltk\n",
        "  e.g. pip install pandas numpy scikit-learn requests tqdm tensorflow nltk\n",
        "- Run: python end_to_end_sentiment_with_datasets.py\n",
        "- If you want to use full Sentiment140, increase SAMPLE_SIZE or set SAMPLE_SIZE = None.\n",
        "\n",
        "DATA SOURCES (auto-download attempted):\n",
        "- Sentiment140 training/test zip: http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
        "- GloVe (manual): download glove.6B.300d.txt from https://nlp.stanford.edu/projects/glove/ and place it in the working dir or set GLOVE_PATH below.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import io\n",
        "import zipfile\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# -------------------------\n",
        "# CONFIG - edit as needed\n",
        "# -------------------------\n",
        "SENT140_URL = \"http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\"\n",
        "WORK_DIR = \"data\"\n",
        "SENT140_ZIP = os.path.join(WORK_DIR, \"trainingandtestdata.zip\")\n",
        "SENT140_CSV = os.path.join(WORK_DIR, \"training.1600000.processed.noemoticon.csv\")\n",
        "SAMPLE_SIZE = 50000     # set to None to use entire dataset (may be very slow/large)\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "GLOVE_PATH = \"glove.6B.300d.txt\"   # place here if you downloaded; otherwise fallback to random init\n",
        "EMBEDDING_DIM = 300\n",
        "MAX_NUM_WORDS = 30000\n",
        "MAX_SEQ_LEN = 50\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 5    # increase if you have GPU/time\n",
        "VERBOSE = 1\n",
        "TEST_SIZE = 0.2\n",
        "VAL_RATIO_FROM_TRAIN = 0.1\n",
        "# -------------------------\n",
        "\n",
        "random.seed(RANDOM_STATE)\n",
        "np.random.seed(RANDOM_STATE)\n",
        "tf.random.set_seed(RANDOM_STATE)\n",
        "\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "\n",
        "def download_file(url, target_path, chunk_size=32768):\n",
        "    if os.path.exists(target_path):\n",
        "        print(f\"[download] already exists: {target_path}\")\n",
        "        return target_path\n",
        "    print(f\"[download] fetching {url} -> {target_path}\")\n",
        "    resp = requests.get(url, stream=True, timeout=60)\n",
        "    resp.raise_for_status()\n",
        "    total = int(resp.headers.get('content-length', 0))\n",
        "    with open(target_path, 'wb') as f:\n",
        "        with tqdm(total=total, unit='B', unit_scale=True) as pbar:\n",
        "            for chunk in resp.iter_content(chunk_size=chunk_size):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "                    pbar.update(len(chunk))\n",
        "    return target_path\n",
        "\n",
        "def extract_sent140(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        # the zip contains training.1600000.processed.noemoticon.csv and testdata.manual.2009.06.14.csv\n",
        "        z.extractall(path=extract_to)\n",
        "    print(\"[extract] done.\")\n",
        "\n",
        "def load_sentiment140(sample_size=SAMPLE_SIZE, random_state=RANDOM_STATE):\n",
        "    # download and extract if not present\n",
        "    try:\n",
        "        if not os.path.exists(SENT140_CSV):\n",
        "            download_file(SENT140_URL, SENT140_ZIP)\n",
        "            extract_sent140(SENT140_ZIP, WORK_DIR)\n",
        "        # file format: sentiment,id,date,query,user,text  (no header)\n",
        "        # sentiment: 0 = negative, 2 = neutral, 4 = positive\n",
        "        print(\"[load] reading Sentiment140 CSV (this may take a while)...\")\n",
        "        df = pd.read_csv(SENT140_CSV, encoding='latin-1', header=None)\n",
        "        df = df[[0,5]]\n",
        "        df.columns = ['sentiment', 'text']\n",
        "        # map 4->1, 0->0; drop neutral 2\n",
        "        df = df[df['sentiment'] != 2].copy()\n",
        "        df['label'] = df['sentiment'].map({0:0, 4:1})\n",
        "        df = df[['text','label']].reset_index(drop=True)\n",
        "        if sample_size is not None and sample_size > 0 and sample_size < len(df):\n",
        "            df = df.sample(n=sample_size, random_state=random_state).reset_index(drop=True)\n",
        "        print(f\"[load] loaded Sentiment140: {len(df)} examples (labels counts: {df['label'].value_counts().to_dict()})\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"[load] failed to load Sentiment140: {e}\")\n",
        "        return None\n",
        "\n",
        "def make_small_sample():\n",
        "    # fallback small dataset (balanced)\n",
        "    print(\"[fallback] creating small built-in sample dataset\")\n",
        "    positive = [\n",
        "        \"I love this product! Totally recommend it :)\",\n",
        "        \"What a great experience, I'm so happy\",\n",
        "        \"Absolutely fantastic service and friendly people\",\n",
        "        \"Best purchase ever, very satisfied\",\n",
        "        \"This made my day, wonderful!\"\n",
        "    ]\n",
        "    negative = [\n",
        "        \"I hate this. Worst ever!\",\n",
        "        \"Terrible experience, will never come back\",\n",
        "        \"Very disappointed, broke after one use\",\n",
        "        \"Waste of money and time\",\n",
        "        \"This ruined my day, awful service\"\n",
        "    ]\n",
        "    texts = positive + negative\n",
        "    labels = [1]*len(positive) + [0]*len(negative)\n",
        "    df = pd.DataFrame({'text': texts, 'label': labels})\n",
        "    return df\n",
        "\n",
        "# -------------------------\n",
        "# Load dataset (attempt Sentiment140, else fallback)\n",
        "# -------------------------\n",
        "df = load_sentiment140()\n",
        "if df is None or len(df) < 100:\n",
        "    df = make_small_sample()\n",
        "\n",
        "# basic cleaning - you can expand (lowercase, remove URLs, mentions, etc.)\n",
        "def simple_clean(text):\n",
        "    txt = str(text)\n",
        "    # minimal cleaning: strip\n",
        "    return txt.strip()\n",
        "\n",
        "df['text_clean'] = df['text'].map(simple_clean)\n",
        "print(\"[data] sample rows:\\n\", df.head().to_dict(orient='records')[:3])\n",
        "\n",
        "# -------------------------\n",
        "# Train / Val / Test split\n",
        "# -------------------------\n",
        "train_df, test_df = train_test_split(df, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=df['label'])\n",
        "train_df, val_df = train_test_split(train_df, test_size=VAL_RATIO_FROM_TRAIN, random_state=RANDOM_STATE, stratify=train_df['label'])\n",
        "\n",
        "print(f\"[split] train={len(train_df)} val={len(val_df)} test={len(test_df)}\")\n",
        "\n",
        "# -------------------------\n",
        "# Tokenize and sequences\n",
        "# -------------------------\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_df['text_clean'].tolist())\n",
        "\n",
        "def texts_to_padded(texts):\n",
        "    seq = tokenizer.texts_to_sequences(texts)\n",
        "    return pad_sequences(seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')\n",
        "\n",
        "X_train = texts_to_padded(train_df['text_clean'])\n",
        "X_val   = texts_to_padded(val_df['text_clean'])\n",
        "X_test  = texts_to_padded(test_df['text_clean'])\n",
        "\n",
        "y_train = train_df['label'].values\n",
        "y_val   = val_df['label'].values\n",
        "y_test  = test_df['label'].values\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
        "print(f\"[tokenizer] vocab_size={len(word_index)}, using num_words={num_words}\")\n",
        "\n",
        "# -------------------------\n",
        "# Load GloVe (if present) and build embedding matrix (no gensim)\n",
        "# -------------------------\n",
        "def load_glove(glove_path, dim=EMBEDDING_DIM):\n",
        "    if not os.path.exists(glove_path):\n",
        "        print(f\"[glove] not found at {glove_path}. Skipping GloVe load.\")\n",
        "        return None\n",
        "    print(f\"[glove] loading {glove_path} ...\")\n",
        "    embeddings_index = {}\n",
        "    with open(glove_path, 'r', encoding='utf8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            parts = line.rstrip().split(' ')\n",
        "            if len(parts) <= dim:\n",
        "                continue\n",
        "            word = parts[0]\n",
        "            try:\n",
        "                coefs = np.asarray(parts[1:], dtype='float32')\n",
        "            except:\n",
        "                continue\n",
        "            if coefs.shape[0] != dim:\n",
        "                continue\n",
        "            embeddings_index[word] = coefs\n",
        "    print(f\"[glove] loaded vectors for {len(embeddings_index)} words.\")\n",
        "    return embeddings_index\n",
        "\n",
        "glove = load_glove(GLOVE_PATH, EMBEDDING_DIM)\n",
        "\n",
        "def build_embedding_matrix(word_index, embeddings_index, num_words, dim):\n",
        "    rng = np.random.default_rng(RANDOM_STATE)\n",
        "    embedding_matrix = rng.normal(loc=0.0, scale=0.01, size=(num_words, dim)).astype('float32')\n",
        "    if embeddings_index is None:\n",
        "        print(\"[embed] no pre-trained embeddings, using random init\")\n",
        "        return embedding_matrix\n",
        "    oov = 0\n",
        "    for word, i in word_index.items():\n",
        "        if i >= num_words:\n",
        "            continue\n",
        "        vec = embeddings_index.get(word)\n",
        "        if vec is None:\n",
        "            vec = embeddings_index.get(word.lower())\n",
        "        if vec is not None:\n",
        "            embedding_matrix[i] = vec\n",
        "        else:\n",
        "            oov += 1\n",
        "    print(f\"[embed] built matrix: {num_words} words, approx OOV={oov}\")\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_matrix = build_embedding_matrix(word_index, glove, num_words, EMBEDDING_DIM)\n",
        "\n",
        "# -------------------------\n",
        "# Model builders (fresh Embedding per model)\n",
        "# -------------------------\n",
        "def make_embedding_layer(embedding_matrix, num_words, dim, input_length, trainable=False):\n",
        "    return Embedding(input_dim=num_words, output_dim=dim, weights=[embedding_matrix], input_length=input_length, trainable=trainable)\n",
        "\n",
        "def build_lstm_model():\n",
        "    emb = make_embedding_layer(embedding_matrix, num_words, EMBEDDING_DIM, MAX_SEQ_LEN, trainable=False)\n",
        "    model = Sequential([\n",
        "        emb,\n",
        "        Bidirectional(LSTM(128)),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_cnn_model():\n",
        "    emb = make_embedding_layer(embedding_matrix, num_words, EMBEDDING_DIM, MAX_SEQ_LEN, trainable=False)\n",
        "    model = Sequential([\n",
        "        emb,\n",
        "        Conv1D(128, 3, activation='relu'),\n",
        "        MaxPooling1D(3),\n",
        "        Conv1D(128, 3, activation='relu'),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_bilstm_model():\n",
        "    inp = Input(shape=(MAX_SEQ_LEN,))\n",
        "    emb = Embedding(input_dim=num_words, output_dim=EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQ_LEN, trainable=False)(inp)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(emb)\n",
        "    x = Bidirectional(LSTM(64))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# -------------------------\n",
        "# Train & evaluate deep models\n",
        "# -------------------------\n",
        "deep_models = {\n",
        "    \"LSTM\": build_lstm_model,\n",
        "    \"CNN\": build_cnn_model,\n",
        "    \"BiLSTM\": build_bilstm_model\n",
        "}\n",
        "\n",
        "deep_results = []\n",
        "\n",
        "for name, builder in deep_models.items():\n",
        "    try:\n",
        "        print(f\"\\n[train] Building & training {name}\")\n",
        "        model = builder()\n",
        "        model.summary(print_fn=lambda s: print(\"[model]\", s))\n",
        "        es = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True, verbose=0)\n",
        "        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[es], verbose=VERBOSE)\n",
        "        probs = model.predict(X_test, batch_size=BATCH_SIZE, verbose=0).ravel()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "        acc = accuracy_score(y_test, preds)\n",
        "        f1m = f1_score(y_test, preds, average='macro')\n",
        "        print(f\"[eval] {name} | Acc={acc:.4f} | F1-macro={f1m:.4f}\")\n",
        "        deep_results.append({'name': name, 'model': model, 'preds': preds, 'probs': probs, 'accuracy': acc, 'f1_macro': f1m})\n",
        "    except Exception as e:\n",
        "        print(f\"[error] Training {name} failed: {e}\")\n",
        "\n",
        "# -------------------------\n",
        "# Traditional ML baselines (TF-IDF + LR, SVM)\n",
        "# -------------------------\n",
        "print(\"\\n[baseline] Training TF-IDF + LogisticRegression and SVM\")\n",
        "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
        "X_train_tfidf = tfidf.fit_transform(train_df['text_clean'])\n",
        "X_val_tfidf = tfidf.transform(val_df['text_clean'])\n",
        "X_test_tfidf = tfidf.transform(test_df['text_clean'])\n",
        "\n",
        "clf_lr = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
        "clf_lr.fit(X_train_tfidf, y_train)\n",
        "preds_lr = clf_lr.predict(X_test_tfidf)\n",
        "acc_lr = accuracy_score(y_test, preds_lr)\n",
        "f1_lr = f1_score(y_test, preds_lr, average='macro')\n",
        "print(f\"[eval] LogisticRegression | Acc={acc_lr:.4f} | F1-macro={f1_lr:.4f}\")\n",
        "\n",
        "clf_svm = LinearSVC(max_iter=5000, random_state=RANDOM_STATE)\n",
        "clf_svm.fit(X_train_tfidf, y_train)\n",
        "preds_svm = clf_svm.predict(X_test_tfidf)\n",
        "acc_svm = accuracy_score(y_test, preds_svm)\n",
        "f1_svm = f1_score(y_test, preds_svm, average='macro')\n",
        "print(f\"[eval] SVM | Acc={acc_svm:.4f} | F1-macro={f1_svm:.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# Summary table\n",
        "# -------------------------\n",
        "rows = []\n",
        "for r in deep_results:\n",
        "    rows.append({'Model': r['name'], 'Accuracy': r['accuracy'], 'F1-macro': r['f1_macro']})\n",
        "rows.extend([\n",
        "    {'Model': 'LogisticRegression', 'Accuracy': acc_lr, 'F1-macro': f1_lr},\n",
        "    {'Model': 'SVM', 'Accuracy': acc_svm, 'F1-macro': f1_svm}\n",
        "])\n",
        "summary_df = pd.DataFrame(rows).sort_values(by='F1-macro', ascending=False).reset_index(drop=True)\n",
        "print(\"\\n[summary]\\n\", summary_df)\n",
        "summary_df.to_csv(\"model_comparison_summary.csv\", index=False)\n",
        "print(\"[saved] model_comparison_summary.csv\")\n",
        "\n",
        "# -------------------------\n",
        "# Error analysis: choose best model by F1-macro (deep preferred if available)\n",
        "# -------------------------\n",
        "chosen = None\n",
        "if deep_results:\n",
        "    best_deep = max(deep_results, key=lambda x: x['f1_macro'])\n",
        "    chosen = best_deep\n",
        "    print(f\"[choose] best deep model = {best_deep['name']}\")\n",
        "else:\n",
        "    # choose best traditional\n",
        "    if f1_lr >= f1_svm:\n",
        "        chosen = {'name': 'LogisticRegression', 'preds': preds_lr, 'probs': None}\n",
        "    else:\n",
        "        chosen = {'name': 'SVM', 'preds': preds_svm, 'probs': None}\n",
        "    print(f\"[choose] best traditional = {chosen['name']}\")\n",
        "\n",
        "preds = chosen['preds']\n",
        "probs = chosen.get('probs', None)\n",
        "\n",
        "test_texts = test_df['text_clean'].tolist()\n",
        "test_labels = test_df['label'].tolist()\n",
        "\n",
        "mis_pos_idx = [i for i, (t, p) in enumerate(zip(test_labels, preds)) if t == 1 and p == 0]\n",
        "mis_neg_idx = [i for i, (t, p) in enumerate(zip(test_labels, preds)) if t == 0 and p == 1]\n",
        "\n",
        "# collect 5 examples each (or fewer if not available)\n",
        "mis_pos_examples = []\n",
        "mis_neg_examples = []\n",
        "\n",
        "for idx in mis_pos_idx[:5]:\n",
        "    mis_pos_examples.append({'index': int(idx), 'text': test_texts[idx], 'true': int(test_labels[idx]), 'pred': int(preds[idx]), 'prob_pos': float(probs[idx]) if probs is not None else None})\n",
        "for idx in mis_neg_idx[:5]:\n",
        "    mis_neg_examples.append({'index': int(idx), 'text': test_texts[idx], 'true': int(test_labels[idx]), 'pred': int(preds[idx]), 'prob_pos': float(probs[idx]) if probs is not None else None})\n",
        "\n",
        "ea_df = pd.DataFrame(mis_pos_examples + mis_neg_examples)\n",
        "ea_df.to_csv(\"error_analysis_samples.csv\", index=False)\n",
        "print(\"[saved] error_analysis_samples.csv (contains up to 5 misclassified positives and 5 misclassified negatives)\")\n",
        "\n",
        "print(\"\\nDone. Files produced:\\n - model_comparison_summary.csv\\n - error_analysis_samples.csv\\nIf you want me to (a) increase SAMPLE_SIZE, (b) switch to full Sentiment140, (c) fine-tune embeddings (trainable=True), or (d) add a transformer (BERT) baseline, tell me which and I'll give updated code.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aa8A32TwnXdr",
        "outputId": "c129a299-453d-44ef-ce1a-7113868f85c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[download] fetching http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip -> data/trainingandtestdata.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81.4M/81.4M [00:01<00:00, 74.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[extract] done.\n",
            "[load] reading Sentiment140 CSV (this may take a while)...\n",
            "[load] loaded Sentiment140: 50000 examples (labels counts: {1: 25014, 0: 24986})\n",
            "[data] sample rows:\n",
            " [{'text': '@chrishasboobs AHHH I HOPE YOUR OK!!! ', 'label': 0, 'text_clean': '@chrishasboobs AHHH I HOPE YOUR OK!!!'}, {'text': '@misstoriblack cool , i have no tweet apps  for my razr 2', 'label': 0, 'text_clean': '@misstoriblack cool , i have no tweet apps  for my razr 2'}, {'text': '@TiannaChaos i know  just family drama. its lame.hey next time u hang out with kim n u guys like have a sleepover or whatever, ill call u', 'label': 0, 'text_clean': '@TiannaChaos i know  just family drama. its lame.hey next time u hang out with kim n u guys like have a sleepover or whatever, ill call u'}]\n",
            "[split] train=36000 val=4000 test=10000\n",
            "[tokenizer] vocab_size=47828, using num_words=30000\n",
            "[glove] not found at glove.6B.300d.txt. Skipping GloVe load.\n",
            "[embed] no pre-trained embeddings, using random init\n",
            "\n",
            "[train] Building & training LSTM\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[model] Model: \"sequential\"\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
            "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
            "│ embedding (Embedding)           │ ?                      │     9,000,000 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ bidirectional (Bidirectional)   │ ?                      │   0 (unbuilt) │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dropout (Dropout)               │ ?                      │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dense (Dense)                   │ ?                      │   0 (unbuilt) │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dropout_1 (Dropout)             │ ?                      │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dense_1 (Dense)                 │ ?                      │   0 (unbuilt) │\n",
            "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
            " Total params: 9,000,000 (34.33 MB)\n",
            " Trainable params: 0 (0.00 B)\n",
            " Non-trainable params: 9,000,000 (34.33 MB)\n",
            "\n",
            "Epoch 1/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 392ms/step - accuracy: 0.5449 - loss: 0.6838 - val_accuracy: 0.6553 - val_loss: 0.6298\n",
            "Epoch 2/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 380ms/step - accuracy: 0.6439 - loss: 0.6334 - val_accuracy: 0.6758 - val_loss: 0.6055\n",
            "Epoch 3/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 382ms/step - accuracy: 0.6611 - loss: 0.6150 - val_accuracy: 0.6770 - val_loss: 0.5947\n",
            "Epoch 4/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 402ms/step - accuracy: 0.6710 - loss: 0.6068 - val_accuracy: 0.6762 - val_loss: 0.5948\n",
            "Epoch 5/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 378ms/step - accuracy: 0.6820 - loss: 0.5965 - val_accuracy: 0.6697 - val_loss: 0.5999\n",
            "[eval] LSTM | Acc=0.6817 | F1-macro=0.6790\n",
            "\n",
            "[train] Building & training CNN\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[model] Model: \"sequential_1\"\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
            "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
            "│ embedding_1 (Embedding)         │ ?                      │     9,000,000 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ conv1d (Conv1D)                 │ ?                      │   0 (unbuilt) │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ max_pooling1d (MaxPooling1D)    │ ?                      │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ conv1d_1 (Conv1D)               │ ?                      │   0 (unbuilt) │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ global_max_pooling1d            │ ?                      │             0 │\n",
            "│ (GlobalMaxPooling1D)            │                        │               │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dense_2 (Dense)                 │ ?                      │   0 (unbuilt) │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dropout_2 (Dropout)             │ ?                      │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dense_3 (Dense)                 │ ?                      │   0 (unbuilt) │\n",
            "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
            " Total params: 9,000,000 (34.33 MB)\n",
            " Trainable params: 0 (0.00 B)\n",
            " Non-trainable params: 9,000,000 (34.33 MB)\n",
            "\n",
            "Epoch 1/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 89ms/step - accuracy: 0.5763 - loss: 0.6679 - val_accuracy: 0.7140 - val_loss: 0.5562\n",
            "Epoch 2/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 91ms/step - accuracy: 0.7166 - loss: 0.5592 - val_accuracy: 0.7385 - val_loss: 0.5183\n",
            "Epoch 3/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 131ms/step - accuracy: 0.7726 - loss: 0.4853 - val_accuracy: 0.7517 - val_loss: 0.5039\n",
            "Epoch 4/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 87ms/step - accuracy: 0.8261 - loss: 0.4058 - val_accuracy: 0.7410 - val_loss: 0.5322\n",
            "Epoch 5/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 90ms/step - accuracy: 0.8775 - loss: 0.3153 - val_accuracy: 0.7455 - val_loss: 0.5933\n",
            "[eval] CNN | Acc=0.7445 | F1-macro=0.7438\n",
            "\n",
            "[train] Building & training BiLSTM\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[model] Model: \"functional_2\"\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
            "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
            "│ input_layer_2 (InputLayer)      │ (None, 50)             │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ embedding_2 (Embedding)         │ (None, 50, 300)        │     9,000,000 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ bidirectional_1 (Bidirectional) │ (None, 50, 256)        │       439,296 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ bidirectional_2 (Bidirectional) │ (None, 128)            │       164,352 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dropout_3 (Dropout)             │ (None, 128)            │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dense_4 (Dense)                 │ (None, 64)             │         8,256 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dense_5 (Dense)                 │ (None, 1)              │            65 │\n",
            "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
            " Total params: 9,611,969 (36.67 MB)\n",
            " Trainable params: 611,969 (2.33 MB)\n",
            " Non-trainable params: 9,000,000 (34.33 MB)\n",
            "\n",
            "Epoch 1/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 611ms/step - accuracy: 0.5589 - loss: 0.6773 - val_accuracy: 0.6547 - val_loss: 0.6161\n",
            "Epoch 2/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 610ms/step - accuracy: 0.6555 - loss: 0.6221 - val_accuracy: 0.6735 - val_loss: 0.5981\n",
            "Epoch 3/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 614ms/step - accuracy: 0.6684 - loss: 0.6085 - val_accuracy: 0.6808 - val_loss: 0.5927\n",
            "Epoch 4/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 608ms/step - accuracy: 0.6756 - loss: 0.5986 - val_accuracy: 0.6890 - val_loss: 0.5858\n",
            "Epoch 5/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 605ms/step - accuracy: 0.6819 - loss: 0.5907 - val_accuracy: 0.6890 - val_loss: 0.5856\n",
            "[eval] BiLSTM | Acc=0.6945 | F1-macro=0.6936\n",
            "\n",
            "[baseline] Training TF-IDF + LogisticRegression and SVM\n",
            "[eval] LogisticRegression | Acc=0.7871 | F1-macro=0.7871\n",
            "[eval] SVM | Acc=0.7682 | F1-macro=0.7682\n",
            "\n",
            "[summary]\n",
            "                 Model  Accuracy  F1-macro\n",
            "0  LogisticRegression    0.7871  0.787097\n",
            "1                 SVM    0.7682  0.768164\n",
            "2                 CNN    0.7445  0.743815\n",
            "3              BiLSTM    0.6945  0.693597\n",
            "4                LSTM    0.6817  0.679048\n",
            "[saved] model_comparison_summary.csv\n",
            "[choose] best deep model = CNN\n",
            "[saved] error_analysis_samples.csv (contains up to 5 misclassified positives and 5 misclassified negatives)\n",
            "\n",
            "Done. Files produced:\n",
            " - model_comparison_summary.csv\n",
            " - error_analysis_samples.csv\n",
            "If you want me to (a) increase SAMPLE_SIZE, (b) switch to full Sentiment140, (c) fine-tune embeddings (trainable=True), or (d) add a transformer (BERT) baseline, tell me which and I'll give updated code.\n"
          ]
        }
      ]
    }
  ]
}